
import os
import numpy as np
import tensorflow as tf
import input_data
import model
from tensorflow.python.framework import graph_util

# you need to change the directories to yours.
train_dir = '/home/ren/PycharmProjects/recognition/data_train0823/'
test_dir = '/home/ren/PycharmProjects/recognition/data_test0823/'
# train_logs_dir = './logs_0820/train/'
# val_logs_dir = './logs_0820/val'

N_CLASSES = 34
IMG_W = 48     # resize the image, if the input image is too large, training will be very slow.
IMG_H = 48
RATIO = 0.0     # take 20% of dataset as validation data
BATCH_SIZE = 64
num_epochs = 201
# CAPACITY = 2000
# MAX_STEP = 10000 # with current parameters, it is suggested to use MAX_STEP>10k
learning_rate = 0.0001 # with current parameters, it is suggested to use learning rate<0.0001
beta =0.001

def training():

    train, train_label, _, _ = input_data.get_files_v2(train_dir, RATIO)
    val, val_label, _, _ = input_data.get_files_v2(test_dir, RATIO)
    # train_batch, train_label_batch = input_data.get_batch(train,
    #                                               train_label,
    #                                               IMG_W,
    #                                               IMG_H,
    #                                               BATCH_SIZE,
    #                                               CAPACITY)
    # val_batch, val_label_batch = input_data.get_batch(val,
    #                                               val_label,
    #                                               IMG_W,
    #                                               IMG_H,
    #                                               BATCH_SIZE,
    #                                               CAPACITY)
    #print ("type(train_batch)=",type(train_batch))
    data_size = len(train)
    model_save_name = 'rec'
    num_batches_per_epoch = int(data_size / BATCH_SIZE)

    x = tf.placeholder(tf.float32, shape=[None, IMG_W, IMG_H, 3],name="input")
    y_ = tf.placeholder(tf.int32, shape=[None],name="y_")
    #images = train_batch
    logits = model.inference(x,N_CLASSES)
    L1 = tf.nn.softmax(logits)
    # regularizer = tf.contrib.layers.l2_regularizer(beta)
    # reg_term = tf.contrib.layers.apply_regularization(regularizer)

    # optimizer paramaters
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=logits) #+ reg_term
    with tf.name_scope('train'):
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(L1, 1), tf.cast(y_, tf.int64))
    # correct_prediction = tf.equal(tf.argmax(L1,1), tf.argmax(y_,1))

    with tf.name_scope('loss'):
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        tf.summary.scalar('loss', accuracy)
    # accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # save model
    saver = tf.train.Saver()
    print('---training---')


    #loss = model.losses(logits, train_label_batch)
    #train_op = model.trainning(loss, learning_rate)
    #acc = model.evaluation(logits, train_label_batch)


    # with tf.Session() as sess:
    #     saver = tf.train.Saver()
    #     sess.run(tf.global_variables_initializer())
    #     coord = tf.train.Coordinator()
    #     threads = tf.train.start_queue_runners(sess= sess, coord=coord)
    #
    #     summary_op = tf.summary.merge_all()
    #     train_writer = tf.summary.FileWriter(train_logs_dir, sess.graph)
    #     # val_writer = tf.summary.FileWriter(val_logs_dir, sess.graph)
    #
    #     try:
    #         for step in np.arange(MAX_STEP):
    #             if coord.should_stop():
    #                     break
    #             tra_images,tra_labels = sess.run([train_batch, train_label_batch])
    #             #print ("tra_images.shape",tra_images.shape)
    #             _, tra_loss, tra_acc = sess.run([train_op, loss, acc],
    #                                             feed_dict={x:tra_images, y_:tra_labels})
    #             summary_str = sess.run(summary_op)
    #             train_writer.add_summary(summary_str)
    #
    #             if step % 50 == 0:
    #                 print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))
    #
    #
    #             # if step % 200 == 0 or (step + 1) == MAX_STEP:
    #             #     val_images, val_labels = sess.run([val_batch, val_label_batch])
    #             #     val_loss, val_acc = sess.run([loss, acc],
    #             #                                  feed_dict={x:val_images, y_:val_labels})
    #             #     print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc*100.0))
    #             #     summary_str = sess.run(summary_op)
    #             #     val_writer.add_summary(summary_str, step)
    #
    #             if (step + 1) == MAX_STEP:
    #                 #checkpoint_path = os.path.join(train_logs_dir, 'model.ckpt')
    #                 #saver.save(sess, checkpoint_path, global_step=step)
    #                 constant_graph =graph_util.convert_variables_to_constants(sess,sess.graph_def,["softmax_linear/softmax_linear_1"])
    #                 with tf.gfile.FastGFile("model/pb/p.pb",mode='wb')as f:
    #                     f.write(constant_graph.SerializeToString())
    #
    #
    #     except tf.errors.OutOfRangeError:
    #         print('Done training -- epoch limit reached')
    #     finally:
    #         coord.request_stop()
    #     coord.join(threads)

    with tf.Session() as sess:
        # 合并到Summary中
        merged = tf.summary.merge_all()
        writer = tf.summary.FileWriter("./model_0820/graph/", sess.graph)
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        for epoch in range(num_epochs):
            # print('epoch:', epoch)
            # shuffle_indices = np.random.permutation(np.arange(data_size))
            # shuffled_data = trainData[shuffle_indices]
            for batch_num in range(num_batches_per_epoch):  # batch数量
                start_index = batch_num * BATCH_SIZE
                end_index = min((batch_num + 1) * BATCH_SIZE, data_size)
                data_batch = train[start_index:end_index]
                label_batch = train_label[start_index:end_index]
                x_data = np.array(data_batch).astype((float))
                #print (x_data.shape)
                y_data = np.array(label_batch).astype(np.int32)  # .astype(np.int64)
                feed = {x: x_data, y_: y_data}
                optimizer.run(feed_dict=feed)
            train_accuracy = accuracy.eval(feed_dict=feed)
            result = sess.run(merged, feed_dict=feed)  # merged也是需要run的
            writer.add_summary(result, epoch)  # result是summary类型的，需要放入writer中，i步数（x轴）
            if epoch % 20 == 0:
                constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def,
                                                                           ["softmax_linear/softmax_linear_1"])
                with tf.gfile.FastGFile("./model_0823/pb/p_%d.pb"%epoch,mode='wb')as f:
                    f.write(constant_graph.SerializeToString())
                saver_path = saver.save(sess, './model_0823/ckpt/' + model_save_name + '_%d.ckpt' % epoch)
            print("epoch %04d | training_accuracy %.6f" % (epoch, train_accuracy))
        print('-----Testing-----')
        x_data = np.array(val)
        y_data = np.array(val_label).astype(np.int32)  # .astype(np.int64)
        feed = {x: x_data, y_: y_data}
        test_num = x_data.shape[0]
        test_accuracy = accuracy.eval(feed_dict=feed)
        print("test_number %04d | testing_accuracy %.9f" % (test_num, test_accuracy))
    print('-+---------------------------+-')
    print("Model saved in file:", saver_path)


def test():
    _, _, val, val_label = input_data.get_files_v2(test_dir, 1)

    val_batch, val_label_batch = input_data.get_batch(val,
                                                      val_label,
                                                      IMG_W,
                                                      IMG_H,
                                                      BATCH_SIZE,
                                                      CAPACITY)
    MAX_STEP = len(val)/BATCH_SIZE
    print (MAX_STEP)
    print(len(val),'/n',len(val_label))

    logits = model.inference( N_CLASSES)
    loss = model.losses(logits, val_label_batch)
    #train_op = model.trainning(loss, learning_rate)
    acc = model.evaluation(logits, val_label_batch)

    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 3])
    y_ = tf.placeholder(tf.int16, shape=[BATCH_SIZE])

    with tf.Session() as sess:
        saver = tf.train.Saver()
        print("Reading checkpoints...")
        ckpt = tf.train.get_checkpoint_state(train_logs_dir)
        if ckpt and ckpt.model_checkpoint_path:
            global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
            saver.restore(sess, ckpt.model_checkpoint_path)
            print('Loading success, global_step is %s' % global_step)
        else:
            print('No checkpoint file found')


        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)

        summary_op = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(train_logs_dir, sess.graph)
        val_writer = tf.summary.FileWriter(val_logs_dir, sess.graph)
        sum_acc = 0.0
        try:
            for step in np.arange(MAX_STEP):
                if coord.should_stop():
                    break
                #tra_images, tra_labels = sess.run([train_batch, train_label_batch])
                #_, tra_loss, tra_acc = sess.run([train_op, loss, acc],
                #                                feed_dict={x: tra_images, y_: tra_labels})

                val_images, val_labels = sess.run([val_batch, val_label_batch])
                val_loss, val_acc = sess.run([loss, acc],
                                             feed_dict={x: val_images, y_: val_labels})
                print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' % (step, val_loss, val_acc * 100.0))
                sum_acc += val_acc
            print  ('the acc = %.2f'%(sum_acc/(step+1)))


        except tf.errors.OutOfRangeError:
            print('Done training -- epoch limit reached')
        finally:
            coord.request_stop()
        coord.join(threads)




#Test one image

def get_one_image(file_dir):
    """
    Randomly pick one image from test data
    Return: ndarray
    """
    from PIL import Image
    #import matplotlib.pyplot as plt
    test =[]
    for file in os.listdir(file_dir):
        test.append(file_dir + file)
    print('There are %d test pictures\n' %(len(test)))

    n = len(test)
    ind = np.random.randint(0, n)
    print(ind)
    img_test = test[ind]
    #img_test = test[1]
    print (img_test)
    image = Image.open(img_test)
    #print  (image)
    #plt.imshow(image)
    image = image.resize([48, 48])
    image = np.array(image)
    return image

def test_one_image():
    """
    Test one image with the saved models and parameters
    """

    test_image = get_one_image(test_dir)

    with tf.Graph().as_default():
        BATCH_SIZE = 1
        N_CLASSES = 34

        image = tf.cast(test_image, tf.float32)
        image = tf.image.per_image_standardization(image)
        image = tf.reshape(image, [1, 48, 48, 3])
        logit = model.inference( N_CLASSES)

        logit = tf.nn.softmax(logit)

        x = tf.placeholder(tf.float32, shape=[48, 48, 3])

        saver = tf.train.Saver()

        with tf.Session() as sess:

            print("Reading checkpoints...")
            ckpt = tf.train.get_checkpoint_state(train_logs_dir)
            if ckpt and ckpt.model_checkpoint_path:
                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
                saver.restore(sess, ckpt.model_checkpoint_path)
                print('Loading success, global_step is %s' % global_step)
            else:
                print('No checkpoint file found')

            prediction = sess.run(logit, feed_dict={x: test_image})
            max_index = np.argmax(prediction)
            print ("max_index is =",max_index )
            # if max_index==0:
            #     print('This is a cat with possibility %.6f' %prediction[:, 0])
            # else:
            #     print('This is a dog with possibility %.6f' %prediction[:, 1])


def ckpt_test():
    ckpt_path = "./model/ckpt"
    result = 0  # evaluate count
    val, val_label, _, _ = input_data.get_files_v2(test_dir, RATIO)
    with tf.Graph().as_default():

        x = tf.placeholder(tf.float32, shape=[None, IMG_W, IMG_H, 3], name="input")
        logit = model.inference(x,34)
        # logit = tf.nn.softmax(logit)
        saver = tf.train.Saver()
        with tf.Session() as sess:

            print("Reading checkpoints...")
            ckpt = tf.train.get_checkpoint_state(ckpt_path)
            if ckpt and ckpt.model_checkpoint_path:
                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
                saver.restore(sess, ckpt.model_checkpoint_path)
                print('Loading success, global_step is %s' % global_step)
            else:
                print('No checkpoint file found')
            for i in range(0,len(val)):
                prediction = sess.run(logit, feed_dict={x: np.reshape(val[i], [1, 48, 48, 3])})
                # print ("***",prediction)
                prediction_labels = np.argmax(prediction, axis=1)
                if prediction_labels == val_label[i]:
                    result+=1
        test_accuracy = result * 1.0 / len(val)
        print("test_number %04d | testing_accuracy %.9f" % (len(val), test_accuracy))

def pb_test():
    result = 0 #evaluate count
    val, val_label, _, _ = input_data.get_files_v2(test_dir, RATIO)
    with tf.Graph().as_default():
        output_graph_def = tf.GraphDef()
        pb_file_path = "./model_0823/pb/p_200.pb"
        with open(pb_file_path, "rb") as f:
            output_graph_def.ParseFromString(f.read())
            _ = tf.import_graph_def(output_graph_def, name="")



        with tf.Session() as sess:

            for op in sess.graph.get_operations():
                print(op.name, op.values())

            init = tf.global_variables_initializer()
            sess.run(init)

            print('-----Testing-----')

            # input_x = sess.graph.get_tensor_by_name("input:0")
            # out_softmax = sess.graph.get_tensor_by_name("softmax:0")
            #
            # out_label = sess.graph.get_tensor_by_name("output:0")

            # test_image = get_one_image(test_dir)
            # img = io.imread(jpg_path)
            # img = transform.resize(img, (224, 224, 3))
            # for i in range(0,len(val)):

                # img_out_softmax = sess.run(out_softmax, feed_dict={input_x: np.reshape(val[i], [-1, 48, 48, 3])})
                # image = tf.cast(test_image, tf.float32)
            # image = tf.image.per_image_standardization(image)
            # image = tf.reshape(image, [1, 48, 48, 3])
            # print (type(image))
            input_x = sess.graph.get_tensor_by_name("input:0")

            # input_x = tf.placeholder(tf.float32, shape=[1,48, 48, 3])
            out_softmax = sess.graph.get_tensor_by_name("softmax_linear/softmax_linear_1:0")
            # image = sess.run(image)
            # print(type(image))
            for i in range(0,len(val)):

                img_out_softmax = sess.run(out_softmax, feed_dict={input_x: np.reshape(val[i], [1, 48, 48, 3])})
                # print ("img_out_softmax:", img_out_softmax)
                prediction_labels = np.argmax(img_out_softmax, axis=1)

                if prediction_labels == val_label[i]:
                    result +=1
                else:
                    print(prediction_labels, " || ", val_label[i])
            # print ("label:", prediction_labels)
            # print (prediction_labels.shape)
        print(result)
        print (len(val))
        test_accuracy = result*1.0/len(val)

    print("test_number %04d | testing_accuracy %.9f" % (len(val), test_accuracy))


if __name__ == '__main__':
    training()
    #test_one_image()
    #test()
    # ckpt_test()
    #pb_test()






